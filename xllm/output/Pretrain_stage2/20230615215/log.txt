{
    "run": {
        "task": "image_text_pretrain",
        "lr_sched": "linear_warmup_cosine_lr",
        "init_lr": 1e-05,
        "min_lr": 1e-08,
        "warmup_lr": 1e-06,
        "weight_decay": 0.05,
        "max_epoch": 2,
        "batch_size_train": 1,
        "batch_size_eval": 4,
        "num_workers": 4,
        "warmup_steps": 6000,
        "accum_grad_iters": 32,
        "seed": 42,
        "output_dir": "output/Pretrain_stage2",
        "amp": true,
        "resume_ckpt_path": null,
        "evaluate": false,
        "train_splits": [
            "train"
        ],
        "device": "cuda",
        "world_size": 2,
        "dist_url": "env://",
        "distributed": true,
        "rank": 0,
        "gpu": 0,
        "dist_backend": "nccl"
    },
    "model": {
        "arch": "xllm_image",
        "load_finetuned": false,
        "finetuned": "",
        "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt6.7b.pth",
        "image_size": 224,
        "drop_path_rate": 0,
        "use_grad_checkpoint": false,
        "vit_precision": "fp16",
        "freeze_vit": true,
        "num_query_token": 32,
        "opt_model": "/raid/cfl/cn_pretraining_multi_dialog/cache/chatglm6b",
        "prompt": "",
        "model_type": "pretrain_xllm_image",
        "load_pretrained": true
    },
    "preprocess": {
        "vis_processor": {
            "train": {
                "name": "blip_image_train",
                "image_size": 224
            },
            "eval": {
                "name": "blip_image_eval",
                "image_size": 224
            }
        },
        "text_processor": {
            "train": {
                "name": "blip_caption"
            },
            "eval": {
                "name": "blip_caption"
            }
        }
    },
    "datasets": {
        "coco_caption_zh_g": {
            "data_type": "images",
            "build_info": {
                "annotations": {
                    "train": {
                        "url": "https://storage.googleapis.com/sfr-vision-language-research/xllm/datasets/visual_genome/vg_caption.json",
                        "storage": "/raid/cfl/en_pretraining/data/images/coco/coco_zh_multi.json"
                    }
                },
                "images": {
                    "storage": "/raid/cfl/en_pretraining/data/images/coco"
                }
            },
            "vis_processor": {
                "train": {
                    "name": "blip2_image_train",
                    "image_size": 224
                }
            },
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            }
        },
        "cc3m_caption_zh_g": {
            "data_type": "images",
            "build_info": {
                "annotations": {
                    "train": {
                        "url": "https://storage.googleapis.com/sfr-vision-language-research/xllm/datasets/visual_genome/vg_caption.json",
                        "storage": "/raid/cfl/en_pretraining/data/images/cc3m/cc3m_zh.json"
                    }
                },
                "images": {
                    "storage": "/raid/cfl/en_pretraining/data/images/cc3m"
                }
            },
            "vis_processor": {
                "train": {
                    "name": "blip2_image_train",
                    "image_size": 224
                }
            },
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            }
        },
        "ai_challenge_zh_g": {
            "data_type": "images",
            "build_info": {
                "annotations": {
                    "train": {
                        "url": "https://storage.googleapis.com/sfr-vision-language-research/xllm/datasets/visual_genome/vg_caption.json",
                        "storage": "/raid/cfl/data/ai_challenge/ai_challenge_multi.json"
                    }
                },
                "images": {
                    "storage": "/raid/cfl/data/ai_challenge"
                }
            },
            "vis_processor": {
                "train": {
                    "name": "blip2_image_train",
                    "image_size": 224
                }
            },
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            }
        },
        "flickr30k_zh_g": {
            "data_type": "images",
            "build_info": {
                "annotations": {
                    "train": {
                        "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/flickr30k_train.json",
                        "storage": "/raid/cfl/data/Flickr30k-CN/Flickr30k_zh_multi.json"
                    }
                },
                "images": {
                    "storage": "flickr30k/images"
                }
            },
            "vis_processor": {
                "train": {
                    "name": "blip2_image_train",
                    "image_size": 224
                }
            },
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            }
        },
        "vg_caption_zh_g": {
            "data_type": "images",
            "build_info": {
                "annotations": {
                    "train": {
                        "url": "https://storage.googleapis.com/sfr-vision-language-research/xllm/datasets/visual_genome/vg_caption.json",
                        "storage": "/raid/cfl/en_pretraining/data/images/vg_zh.json"
                    }
                },
                "images": {
                    "storage": "/raid/cfl/en_pretraining/data/images/VG"
                }
            },
            "vis_processor": {
                "train": {
                    "name": "blip2_image_train",
                    "image_size": 224
                }
            },
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            }
        },
        "sbu_caption_zh_g": {
            "data_type": "images",
            "build_info": {
                "annotations": {
                    "train": {
                        "url": [
                            "https://storage.googleapis.com/sfr-vision-language-research/xllm/datasets/sbu/sbu.json"
                        ],
                        "storage": [
                            "/raid/cfl/en_pretraining/data/images/sbu/sbu_zh.json"
                        ]
                    }
                },
                "images": {
                    "storage": "/raid/cfl/en_pretraining/data/images/sbu/pythonDownload/subpic"
                }
            },
            "vis_processor": {
                "train": {
                    "name": "blip2_image_train",
                    "image_size": 224
                }
            },
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            }
        },
        "wukong_caption_zh_g": {
            "data_type": "images",
            "build_info": {
                "annotations": {
                    "train": {
                        "url": "https://storage.googleapis.com/sfr-vision-language-research/xllm/datasets/visual_genome/vg_caption.json",
                        "storage": "/raid/cfl/data/wukong/caption-image-total-1000w-fliter.json"
                    }
                },
                "images": {
                    "storage": "/raid/cfl/data/wukong/image_total"
                }
            },
            "vis_processor": {
                "train": {
                    "name": "blip2_image_train",
                    "image_size": 224
                }
            },
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            }
        }
    }
}
